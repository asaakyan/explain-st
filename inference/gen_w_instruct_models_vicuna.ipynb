{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/a.saakyan/tmp/ENTER/envs/alpaca/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "os.environ['HF_HOME'] =  \"/mnt/swordfish-pool2/models/\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2,3\"\n",
    "from transformers import AutoTokenizer, LlamaForCausalLM, GenerationConfig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vicuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [02:02<00:00, 40.83s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 5120, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-39): 40 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (k_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (v_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (o_proj): Linear(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "          (down_proj): Linear(in_features=13824, out_features=5120, bias=False)\n",
       "          (up_proj): Linear(in_features=5120, out_features=13824, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=5120, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "dir_of_hf_w = \"/mnt/swordfish-pool2/models/vicuna/13B\"\n",
    "print(\"Loading the model and tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(dir_of_hf_w)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    dir_of_hf_w,\n",
    "    load_in_8bit=False,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(tokenizer, model, \n",
    "             input, instruction=None, fs_prompt=None, \n",
    "             verbose=False):\n",
    "    prompt = generate_prompt(input, instruction, fs_prompt)\n",
    "    if verbose: \n",
    "        print(\"****PROMPT:****\")\n",
    "        print(prompt)\n",
    "        print(\"****END PROMPT****\")\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].cuda() # these are integers encoded from words\n",
    "    generation_config = GenerationConfig(\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=generation_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=256,\n",
    "    )\n",
    "    s = generation_output.sequences[0]\n",
    "    output = tokenizer.decode(s)\n",
    "    # return output.split(\"### Response:\")[1].replace(\"</s>\", \"\").strip()\n",
    "    return output.strip().replace(\"</s>\", \"\").replace(\"<s>\", \"\")\n",
    "\n",
    "def construct_few_shot_prompt(train_file_path, n=10):\n",
    "    # prompt = \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\"\n",
    "    # sample 10 instances from train.json\n",
    "    with open(train_file_path, \"r\") as f:\n",
    "        train_samples = json.load(f)\n",
    "        train_samples = train_samples[:n]\n",
    "    for i, row in enumerate(train_samples):\n",
    "        instruction, input, response = row[\"instruction\"], row[\"input\"], row[\"output\"]\n",
    "        if i ==0: prompt = f\"\"\"{instruction}\\nFor example:\\n\"\"\"\n",
    "        prompt += f\"\"\"{input}\n",
    "{response}\n",
    "\n",
    "\"\"\"\n",
    "    prompt = f\"{prompt}For the following sentence, {instruction.lower()}\"\n",
    "    return prompt.strip()\n",
    "\n",
    "def generate_prompt(input, instruction=None, fs_prompt=None):\n",
    "    if input and instruction:\n",
    "        return f\"\"\"{instruction}\n",
    "{input}\"\"\"\n",
    "    if input and fs_prompt:\n",
    "        return f\"\"\"{fs_prompt}\n",
    "        \n",
    "{input}\"\"\"\n",
    "\n",
    "def run_inference(input_json, out_dir,\n",
    "                   model_name, model, tokenizer,\n",
    "                   n_shots = True, train_file_path=None,\n",
    "                   verbose=False, stop_idx=10000):\n",
    "    \n",
    "    output = []\n",
    "    with open( input_json,'r') as fp:\n",
    "        test_file = json.load(fp)\n",
    "        \n",
    "    for i, line in enumerate(test_file):\n",
    "        if i > stop_idx: break\n",
    "\n",
    "        print(f\"INSTANCE {i}, percentage: {i/len(test_file)}\")\n",
    "        print(f\"INSTRUCTION: {line['instruction']}\\nINPUT: {line['input']}\\n\")\n",
    "\n",
    "        if n_shots == 0:\n",
    "            pred = evaluate(tokenizer, model, \n",
    "                            input=line['input'], instruction=line['instruction'], \n",
    "                            verbose=verbose)\n",
    "        else:\n",
    "            fs_prompt = construct_few_shot_prompt(train_file_path, n=n_shots)\n",
    "            pred = evaluate(tokenizer, model, \n",
    "                            input=line['input'], fs_prompt=fs_prompt,\n",
    "                            verbose=verbose)\n",
    "            \n",
    "        output += [{\"instruction\": line['instruction'],\n",
    "                \"input\": line['input'],\n",
    "                \"pred\": pred,\n",
    "                \"id\": i\n",
    "                }]\n",
    "        \n",
    "        print(\"RESPONSE:\\n\", pred)\n",
    "        print(\"-\"*50)\n",
    "\n",
    "        if i%10==0:\n",
    "            with open(f\"{out_dir}/{model_name}_test_output_max2.json\",'w', encoding='utf-8') as fp:\n",
    "                json.dump(output, fp, indent=4)\n",
    "    # final save\n",
    "    with open(f\"{out_dir}/{model_name}_test_output_max2.json\",'w', encoding='utf-8') as fp:\n",
    "            json.dump(output, fp, indent=4)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_dir = \"../data/gyafc_w_ICHF_alpaca/informal_to_formal\"\n",
    "# assert os.path.isdir(out_dir)\n",
    "# input_json = \"../data/gyafc_w_ICHF_alpaca/informal_to_formal/test.json\"\n",
    "# assert os.path.isfile(input_json)\n",
    "# train_file_path = \"../data/gyafc_w_ICHF_alpaca/informal_to_formal/train.json\"\n",
    "# assert os.path.isfile(train_file_path)\n",
    "\n",
    "# # fs_prompt = construct_few_shot_prompt(train_file_path, n=3)\n",
    "# # print(generate_prompt(input=\"test\", fs_prompt=fs_prompt))\n",
    "\n",
    "# run_inference(input_json, out_dir, \n",
    "#               \"vicuna\", model, tokenizer, \n",
    "#               n_shots=1, train_file_path=train_file_path,\n",
    "#               verbose=False, stop_idx=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"../data/gyafc_w_ICHF_alpaca/formal_to_informal\"\n",
    "assert os.path.isdir(out_dir)\n",
    "input_json = \"../data/gyafc_w_ICHF_alpaca/formal_to_informal/test.json\"\n",
    "assert os.path.isfile(input_json)\n",
    "train_file_path = \"../data/gyafc_w_ICHF_alpaca/formal_to_informal/train.json\"\n",
    "assert os.path.isfile(train_file_path)\n",
    "\n",
    "run_inference(input_json, out_dir, \n",
    "              \"vicuna\", model, tokenizer, \n",
    "              n_shots=1, train_file_path=train_file_path,\n",
    "              verbose=False, stop_idx=10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alpaca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
